# @package optimizer
# https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html

_target_: torch.optim.Adagrad

params: ???
lr: ${lr} # default: 0.01
lr_decay: 0
weight_decay: 0
initial_accumulator_value: 0
eps: 1e-10